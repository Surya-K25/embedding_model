======================================================================
DATA LOADING
======================================================================
Corpus Statistics:
  total_sentences: 17,006
  total_words: 17,005,207
  unique_words: 253,854
  avg_words_per_sentence: 1,000

Sample sentences:
  1. anarchism originated as a term of abuse first used against early working class radicals including...
  2. american individualist anarchism benjamin tucker in one eight two five josiah warren had participated in...

======================================================================
BPE TOKENIZATION
======================================================================
Vocabulary size: 12000

Vocabulary sample (first 20 tokens):
   236: have
   839: light
  11094: nl
  11729: baghd
  2332: ensive
  3342: regional
  10132: hearts
  11068: challenges
  7229: pm
    71: ct
  6253: maxim
  3627: cav
  3789: crus
  1539: round
  7927: ogue
  4308: acting
  5764: sn
  6693: colorado
  9183: hugo
  2978: labour

Tokenization test:
  Text: the king and queen lived in a beautiful castle
  Encoded: [35, 261, 47, 2190, 2863, 31, 4, 5936, 4370]...
  Tokens: ['the', 'king', 'and', 'queen', 'lived', 'in', 'a', 'beautiful', 'castle']

======================================================================
CORPUS TOKENIZATION
======================================================================
Loading existing tokenized corpus...
Total tokenized sentences: 17006
Sample: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radical', 's']

======================================================================
MODEL TRAINING
======================================================================

Training Skip-gram Model
Vector size: 128
Window size: 5
Minimum count: 5
Negative samples: 10
Epochs: 10
Workers: 12
Training sentences: 17006
============================================================
============================================================
Training completed in 522.52 seconds
Vocabulary size: 11896
============================================================
Model saved to models\word2vec_skipgram.model
Word vectors saved to models\word2vec_vectors.kv
Training info saved to models\training_info.json

Model vocabulary size: 11896
Sample words: ['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'is', 'two', 's', 'as', 'eight', 'for', 'five', 'by', 'three', 'was', 'that', 'four', 'on', 'six', 'seven', 'with', 'or', 'it', 'an', 'are', 'from']

======================================================================
SIMILARITY TESTS
======================================================================

Similarity Scores:
Word 1          Word 2          Expected             Score
----------------------------------------------------------------------
king            queen           HIGH - royalty       0.6393
man             woman           HIGH - gender        0.5926
good            better          HIGH - comparative   0.5095
walk            walking         HIGH - verb forms    0.6108
king            man             MEDIUM - gender-royalty 0.3318
good            bad             MEDIUM - antonyms    0.6287
king            computer        LOW - unrelated      0.1017
happy           tree            LOW - unrelated      0.2960

======================================================================
ANALOGY TESTS
======================================================================

Analogy Solving Results:

Gender-Royalty:
  man:king :: woman:? (expected: queen)
  ✓ Correct! Top predictions:
    ★ 1. queen                (similarity: 0.6354)
      2. regent               (similarity: 0.5798)
      3. child                (similarity: 0.5525)
      4. princess             (similarity: 0.5490)
      5. throne               (similarity: 0.5449)

Gender-Royalty Reverse:
  man:woman :: king:? (expected: queen)
  ✓ Correct! Top predictions:
    ★ 1. queen                (similarity: 0.6354)
      2. regent               (similarity: 0.5798)
      3. child                (similarity: 0.5525)
      4. princess             (similarity: 0.5490)
      5. throne               (similarity: 0.5449)

Past Tense:
  walk:walked :: talk:? (expected: talked)
  ✗ Cannot solve: ['walked'] not in vocabulary

Comparative:
  good:better :: bad:? (expected: worse)
  ✓ Correct! Top predictions:
    ★ 1. worse                (similarity: 0.5585)
      2. poorly               (similarity: 0.5029)
      3. faster               (similarity: 0.4907)
      4. slower               (similarity: 0.4887)
      5. saf                  (similarity: 0.4826)

Country-Capital:
  france:paris :: england:? (expected: london)
  ✓ Correct! Top predictions:
    ★ 1. london               (similarity: 0.6947)
      2. edinburgh            (similarity: 0.6047)
      3. manchester           (similarity: 0.5839)
      4. philadelphia         (similarity: 0.5743)
      5. scotland             (similarity: 0.5741)

Comparative Size:
  big:bigger :: small:? (expected: smaller)
  ✗ Cannot solve: ['bigger'] not in vocabulary

======================================================================
 STEP 5C: K-NEAREST NEIGHBORS
======================================================================

Most similar to 'king':
  1. iii                  0.6729
  2. prince               0.6723
  3. crowned              0.6704
  4. queen                0.6393
  5. vii                  0.6299
  6. kings                0.6141
  7. regent               0.6092
  8. henry                0.6032

Most similar to 'computer':
  1. computers            0.7367
  2. hardware             0.7115
  3. computing            0.6732
  4. software             0.6452
  5. graphics             0.6353
  6. video                0.6352
  7. interface            0.6310
  8. technology           0.6296

Most similar to 'happy':
  1. quiet                0.6065
  2. love                 0.5755
  3. loved                0.5417
  4. friends              0.5404
  5. pret                 0.5393
  6. married              0.5386
  7. luck                 0.5377
  8. happ                 0.5365

Most similar to 'one':
  1. seven                0.9651
  2. eight                0.9591
  3. four                 0.9549
  4. six                  0.9537
  5. nine                 0.9446
  6. five                 0.9441
  7. three                0.9441
  8. two                  0.9235

Most similar to 'big':
  1. bang                 0.5606
  2. ger                  0.5251
  3. baby                 0.5038
  4. blue                 0.4904
  5. black                0.4828
  6. classic              0.4797
  7. little               0.4774
  8. boy                  0.4749

Most similar to 'red':
  1. blue                 0.6531
  2. white                0.6508
  3. yellow               0.6301
  4. cres                 0.6125
  5. black                0.6007
  6. colored              0.5926
  7. purple               0.5674
  8. green                0.5545

======================================================================
 STEP 6: VISUALIZATIONS
======================================================================

1. Creating t-SNE visualization (top 100 words)...
Creating t-SNE visualization...
Visualizing 100 words
   ✗ Error: TSNE.__init__() got an unexpected keyword argument 'n_iter'

2. Creating UMAP visualization (top 100 words)...
Creating UMAP visualization...
Visualizing 100 words
C:\Users\SuryabhaasIndranilKa\Desktop\embedding_model\env\Lib\site-packages\umap\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Visualization saved to output/umap_embeddings.png
   ✓ Saved to output/umap_embeddings.png

3. Creating semantic clusters visualization...
Visualizing 30 words from 5 groups
   ✗ Error: perplexity (30.0) must be less than n_samples (30)

======================================================================
 SAVING RESULTS SUMMARY
======================================================================
Results summary saved to output\results_summary.json

======================================================================
 PIPELINE COMPLETED
======================================================================
Total execution time: 551.04 seconds (9.2 minutes)

Outputs saved to:
  - models/          (trained model and tokenizer)
  - output/          (visualizations and results)
  - data/            (corpus and tokenized data)

======================================================================
SUCCESS! Your custom word embedding model is ready to use.
======================================================================